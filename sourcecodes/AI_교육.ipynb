{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "96xslWjcvEVs"
      },
      "source": [
        "# 수치미분\n",
        "1. 함수 f(x) 정의\n",
        "2. 델타 x를 작은 값으로 설정(1e-4)\n",
        "3. 분자/분모 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHQt7LOXTWbh",
        "outputId": "76d2f0fb-ec14-4203-f416-2bef7c7de124"
      },
      "outputs": [],
      "source": [
        "def nummerical_derivative(f, x):\n",
        "  delta_x = 1e-4\n",
        "  return (f(x+delta_x) - f(x-delta_x)) / (2*delta_x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G71MMezewGNt"
      },
      "source": [
        "# 수치미분 최종(다변수)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dqo63O2wxx_Q"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumerical_derivative\u001b[39m(f,x): \u001b[39m# f: 다변수 함수, x: 모든 변수를 포함하고 있는 numpy 객체\u001b[39;00m\n\u001b[0;32m      4\u001b[0m   delta_x \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def numerical_derivative(f,x): # f: 다변수 함수, x: 모든 변수를 포함하고 있는 numpy 객체\n",
        "  delta_x = 1e-4\n",
        "  grad = np.zeros_like(x) # 계산된 수치미분 값 저장 변수\n",
        "  # zeros_like는 x 변수만큼의 사이즈를 가지는 배열을 생성함(0으로 초기화)\n",
        "\n",
        "  #flags = ['multi_index'] : (row, col) 형태 => 행렬을 쓰기 때문\n",
        "  # op_flags = ['readwrite'] : 읽고 쓰기 형태\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) # 모든 입력변수에 대해 편미분하기 위해 iterator 획득\n",
        "\n",
        "\n",
        "  while not it.finished: # 변수의 개수만큼 반복\n",
        "    idx = it.multi_index\n",
        "\n",
        "    tmp_val = x[idx] # numpy 타입은 mutable 이므로 원래 값 보관\n",
        "\n",
        "    # 하나의 변수에 대한 수치미분 계산(기존코드와 동일)\n",
        "    #########################################\n",
        "    x[idx] = float(tmp_val) + delta_x\n",
        "    fx1 = f(x)\n",
        "\n",
        "    x[idx] = float(tmp_val) - delta_x\n",
        "    fx2 = f(x)\n",
        "    grad[idx] = (fx1-fx2) / (2*delta_x)\n",
        "    ##########################################\n",
        "    x[idx] = tmp_val\n",
        "    it.iternext()\n",
        "\n",
        "  return grad"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5vFTvblACME"
      },
      "source": [
        "# 경사하강법"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf6z8_5YAFoK"
      },
      "source": [
        "## 1. simple regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be5Tu41HAK5c"
      },
      "outputs": [],
      "source": [
        "# 학습데이터 준비\n",
        "import numpy as np\n",
        "\n",
        "x_data = np.array([1,2,3,4,5]).reshape(5,1) # 입력 데이터\n",
        "y_data = np.array([2,3,4,5,6]).reshape(5,1) # 정답 데이터\n",
        "\n",
        "t_data = np.array([8,9,10,11,12]).reshape(5,1) # x_data, y_data와 크기가 같아야 함.(행렬)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL9eesH7dCbX",
        "outputId": "ada3e670-5cb8-43e2-a636-eb3046f3751c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W=  [[0.01172554]] , W.shape=  (1, 1) , b =  [0.03979369] , b.shape =  (1,)\n"
          ]
        }
      ],
      "source": [
        "# 임의의 직선 y=Wx+b 정의(임의의 값으로 가중치 W, 바이어스 b 초기화)\n",
        "\n",
        "W = np.random.rand(1,1)\n",
        "b = np.random.rand(1)\n",
        "print(\"W= \", W, \", W.shape= \", W.shape, \", b = \", b, \", b.shape = \", b.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYFMeVtGdTit"
      },
      "outputs": [],
      "source": [
        "# 손실함수 E(W,b) 정의\n",
        "def loss_func(x, t):\n",
        "  y = np.dot(x, W) + b\n",
        "  return (np.sum((t-y)**2) / len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tVL-OJldy_F"
      },
      "outputs": [],
      "source": [
        "# 수치미분 numerical_derivative 및 utility 함수 정의\n",
        "\n",
        "def numerical_derivative(f,x): # f: 다변수 함수, x: 모든 변수를 포함하고 있는 numpy 객체\n",
        "  delta_x = 1e-4\n",
        "  grad = np.zeros_like(x) # 계산된 수치미분 값 저장 변수\n",
        "  # zeros_like는 x 변수만큼의 사이즈를 가지는 배열을 생성함(0으로 초기화)\n",
        "\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) # 모든 입력변수에 대해 편미분하기 위해 iterator 획득\n",
        "\n",
        "\n",
        "  while not it.finished: # 변수의 개수만큼 반복\n",
        "    idx = it.multi_index\n",
        "\n",
        "    tmp_val = x[idx] # numpy 타입은 mutable 이므로 원래 값 보관\n",
        "\n",
        "    # 하나의 변수에 대한 수치미분 계산(기존코드와 동일)\n",
        "    #########################################\n",
        "    fx1 = f(x)\n",
        "\n",
        "    x[idx] = tmp_val - delta_x\n",
        "    fx2 = f(x)\n",
        "    grad[idx] = (fx1-fx2) / (2*delta_x)\n",
        "    ##########################################\n",
        "    x[idx] = tmp_val\n",
        "    it.iternext()\n",
        "\n",
        "  return grad\n",
        "\n",
        "# 손실함수 값 계산 함수\n",
        "# 입력변수 x, t : numpy type\n",
        "def error_val(x,t):\n",
        "  y = np.dot(x,W) + b\n",
        "  return (np.sum((t-y)**2) / len(x))\n",
        "\n",
        "def predict(x):\n",
        "  y = np.dot(x,W) + b\n",
        "\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyUlo-LpeTzO",
        "outputId": "e3a0a345-1c9e-4c55-e544-80f3e9a46612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial error value =  5.213422150974519 initial W =  [[2.48234479]] \n",
            " , b =  [1.64812863]\n",
            "step =  0 error value =  5.213245893519533 W =  [[2.48231982]] , b =  [1.64821912]\n",
            "step =  400 error value =  5.143218719280196 W =  [[2.47236077]] , b =  [1.68429305]\n",
            "step =  800 error value =  5.074132248803851 W =  [[2.46245753]] , b =  [1.72012072]\n",
            "step =  1200 error value =  5.005973833640657 W =  [[2.45261399]] , b =  [1.75570497]\n",
            "step =  1600 error value =  4.938731003943064 W =  [[2.44283242]] , b =  [1.79104819]\n",
            "step =  2000 error value =  4.872391460105987 W =  [[2.43311403]] , b =  [1.82615244]\n",
            "step =  2400 error value =  4.806943068411382 W =  [[2.42345945]] , b =  [1.86101964]\n",
            "step =  2800 error value =  4.742373858148936 W =  [[2.41386887]] , b =  [1.89565154]\n",
            "step =  3200 error value =  4.678672019268039 W =  [[2.40434227]] , b =  [1.93004986]\n",
            "step =  3600 error value =  4.615825900216465 W =  [[2.39487947]] , b =  [1.96421623]\n",
            "step =  4000 error value =  4.553824005846495 W =  [[2.38548018]] , b =  [1.99815225]\n",
            "step =  4400 error value =  4.492654995349951 W =  [[2.37614409]] , b =  [2.03185952]\n",
            "step =  4800 error value =  4.432307680213056 W =  [[2.36687082]] , b =  [2.06533958]\n",
            "step =  5200 error value =  4.372771022190131 W =  [[2.35765998]] , b =  [2.09859398]\n",
            "step =  5600 error value =  4.314034131297733 W =  [[2.34851119]] , b =  [2.13162424]\n",
            "step =  6000 error value =  4.256086263831689 W =  [[2.33942403]] , b =  [2.16443188]\n",
            "step =  6400 error value =  4.198916820407018 W =  [[2.3303981]] , b =  [2.19701841]\n",
            "step =  6800 error value =  4.142515344022784 W =  [[2.321433]] , b =  [2.22938531]\n",
            "step =  7200 error value =  4.086871518151405 W =  [[2.31252831]] , b =  [2.26153406]\n",
            "step =  7600 error value =  4.031975164852777 W =  [[2.30368363]] , b =  [2.29346614]\n",
            "step =  8000 error value =  3.977816242913435 W =  [[2.29489856]] , b =  [2.32518301]\n"
          ]
        }
      ],
      "source": [
        "# learning rate 초기화 및 손실함수가 최소가 될 때까지 W, b 업데이트\n",
        "learning_rate = 1e-4 # 발산하는 경우 1e-3~1e-6 등으로 바꾸어서 실행\n",
        "\n",
        "f = lambda x : loss_func(x_data, t_data) # f(x) = loss_func(x_data, t_data)\n",
        "\n",
        "print(\"initial error value = \", error_val(x_data, t_data), \"initial W = \", W, \"\\n\", \", b = \", b)\n",
        "for step in range(8001):\n",
        "  W-=learning_rate * numerical_derivative(f, W)\n",
        "  b-=learning_rate * numerical_derivative(f, b)\n",
        "\n",
        "  if(step % 400 == 0):\n",
        "    print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re8vbszLh3mN",
        "outputId": "9024ce72-6196-4c56-b642-1de22d8102bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[231.81503869],\n",
              "       [224.93034302],\n",
              "       [188.21196611],\n",
              "       [167.5578791 ],\n",
              "       [151.4935892 ]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(np.array([100,97,81,72,65]).reshape(5,1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h3UZ0LTBgKaX"
      },
      "source": [
        "# 2. multi-variable regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRSABsTpesVz"
      },
      "outputs": [],
      "source": [
        "# 학습데이터 준비\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# x_data\n",
        "# t_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pqbqskeg0Wd",
        "outputId": "4d6ec638-7725-4454-a5d0-fa0743173b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W =  [[0.22555698]\n",
            " [0.63412545]\n",
            " [0.16786301]] , W.shape =  (3, 1) , b =  [0.19565637] , b.shape =  (1,)\n"
          ]
        }
      ],
      "source": [
        "# 임의의 직선 y = W1x1 + W2x2 + ..... + b 정의\n",
        "\n",
        "W = np.random.rand(3,1) # 3x1 행렬\n",
        "b = np.random.rand(1)\n",
        "\n",
        "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-idZM6GhhDhV"
      },
      "outputs": [],
      "source": [
        "# 손실함수 E(W,b) 정의\n",
        "def loss_func(x,t):\n",
        "  y = np.dot(x,W) + b\n",
        "\n",
        "  return (np.sum((t-y)**2) / len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4cn89XKhIko"
      },
      "outputs": [],
      "source": [
        "# 수치미분 numerical_derivative 및 utility 함수 정의\n",
        "\n",
        "def numerical_derivative(f,x): # f: 다변수 함수, x: 모든 변수를 포함하고 있는 numpy 객체\n",
        "  delta_x = 1e-4\n",
        "  grad = np.zeros_like(x) # 계산된 수치미분 값 저장 변수\n",
        "  # zeros_like는 x 변수만큼의 사이즈를 가지는 배열을 생성함(0으로 초기화)\n",
        "\n",
        "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) # 모든 입력변수에 대해 편미분하기 위해 iterator 획득\n",
        "\n",
        "\n",
        "  while not it.finished: # 변수의 개수만큼 반복\n",
        "    idx = it.multi_index\n",
        "\n",
        "    tmp_val = x[idx] # numpy 타입은 mutable 이므로 원래 값 보관\n",
        "\n",
        "    # 하나의 변수에 대한 수치미분 계산(기존코드와 동일)\n",
        "    #########################################\n",
        "    fx1 = f(x)\n",
        "\n",
        "    x[idx] = tmp_val - delta_x\n",
        "    fx2 = f(x)\n",
        "    grad[idx] = (fx1-fx2) / (2*delta_x)\n",
        "    ##########################################\n",
        "    x[idx] = tmp_val\n",
        "    it.iternext()\n",
        "\n",
        "  return grad\n",
        "\n",
        "# 손실함수 값 계산 함수\n",
        "# 입력변수 x, t : numpy type\n",
        "def error_val(x,t):\n",
        "  y = np.dot(x,W) + b\n",
        "  return (np.sum((t-y)**2) / len(x))\n",
        "\n",
        "def predict(x):\n",
        "  y = np.dot(x,W) + b\n",
        "\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "H0Npzh-_hdI_",
        "outputId": "d6ab1777-f5bb-4d13-dd27-60f72e104886"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-1675333972b9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x) = loss_func(x_data, t_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initial error value = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"initial W = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", b = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mW\u001b[0m\u001b[0;34m-=\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumerical_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-08d2268e4ebd>\u001b[0m in \u001b[0;36merror_val\u001b[0;34m(x, t)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 입력변수 x, t : numpy type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0merror_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (5,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)"
          ]
        }
      ],
      "source": [
        "# learning rate 초기화 및 손실함수가 최소가 될 때까지 W, b 업데이트\n",
        "learning_rate = 1e-2 # 발산하는 경우 1e-3~1e-6 등으로 바꾸어서 실행\n",
        "\n",
        "f = lambda x : loss_func(x_data, t_data) # f(x) = loss_func(x_data, t_data)\n",
        "\n",
        "print(\"initial error value = \", error_val(x_data, t_data), \"initial W = \", W, \"\\n\", \", b = \", b)\n",
        "for step in range(8001):\n",
        "  W-=learning_rate * numerical_derivative(f, W)\n",
        "  b-=learning_rate * numerical_derivative(f, b)\n",
        "\n",
        "  if(step % 400 == 0):\n",
        "    print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC31_qCwhe_y"
      },
      "outputs": [],
      "source": [
        "predict(np.array([100,98,81]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simple linear regression과 multiple linear regression을 sklearn으로 구현하는 방법 똑같다.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(x, y)\n",
        "\n",
        "print(model.coef_) # 각 feature들의 coefficients 구하기\n",
        "print(model.predict(x)) # 예측하기\n",
        "print(model.score(x, y)) # 정확도 구하기"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7zD2_X4AiEi_"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gnhb2IrkpMWf"
      },
      "source": [
        "## simple logistic Regression\n",
        "\n",
        "1. 시그모이드 함수와 손실함수(cross-entropy) 구현\n",
        "2. 학습률 a\n",
        "3. W, b 조정 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2nwZJQ44iHMI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "W = np.random.rand(3,1)\n",
        "b = np.random.rand(1,1)\n",
        "\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def loss_func(x,t):\n",
        "    delta = 1e-6\n",
        "    \n",
        "    z = np.dot(x,W) + b\n",
        "    y = sigmoid(z)\n",
        "    \n",
        "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log(1+y+delta)) # 여기서 delta는 log 안이 0이 되는 것을 방지해주는 역할\n",
        "\n",
        "def numerical_derivative(f,x):\n",
        "    delta = 1e-4\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    \n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        \n",
        "        tmp_val = x[idx]\n",
        "        \n",
        "        x[idx] = tmp_val + delta\n",
        "        fx1 = f(x)\n",
        "        \n",
        "        x[idx] = tmp_val - delta\n",
        "        fx2 = f(x)\n",
        "        \n",
        "        grad[idx] = (fx1 - fx2) / (2*delta)\n",
        "        \n",
        "    return grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x : loss_func(x_data, y_data)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5000\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     W \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate\u001b[39m*\u001b[39mnumerical_derivative(f,W)\n\u001b[0;32m      5\u001b[0m     b \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate\u001b[39m*\u001b[39mnumerical_derivative(f,b)\n",
            "Cell \u001b[1;32mIn[7], line 33\u001b[0m, in \u001b[0;36mnumerical_derivative\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m tmp_val \u001b[39m=\u001b[39m x[idx]\n\u001b[0;32m     32\u001b[0m x[idx] \u001b[39m=\u001b[39m tmp_val \u001b[39m+\u001b[39m delta\n\u001b[1;32m---> 33\u001b[0m fx1 \u001b[39m=\u001b[39m f(x)\n\u001b[0;32m     35\u001b[0m x[idx] \u001b[39m=\u001b[39m tmp_val \u001b[39m-\u001b[39m delta\n\u001b[0;32m     36\u001b[0m fx2 \u001b[39m=\u001b[39m f(x)\n",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x : loss_func(x_data, y_data)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5000\u001b[39m):\n\u001b[0;32m      4\u001b[0m     W \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate\u001b[39m*\u001b[39mnumerical_derivative(f,W)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'x_data' is not defined"
          ]
        }
      ],
      "source": [
        "f = lambda x : loss_func(x_data, y_data)\n",
        "\n",
        "for step in range(5000):\n",
        "    W -= learning_rate*numerical_derivative(f,W)\n",
        "    b -= learning_rate*numerical_derivative(f,b)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nPLPEYospRSI"
      },
      "source": [
        "## multi-variable logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2I6atBHpWLs"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# logistic regression with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(train_features, train_labels) # 모델 훈련하기\n",
        "\n",
        "print(model.score(train_features, train_labels)) # 정확도 출력하기\n",
        "\n",
        "print(model.coef_) # 각 feature들의 coefficients 확인하기\n",
        "\n",
        "data = scaler.transform(data) # 로지스틱 회귀를 할 때엔 스케일링을 해줘야 한다. \n",
        "\n",
        "\n",
        "print(model.predict(data)) # 예측하기"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "redjeju",
      "language": "python",
      "name": "conda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
